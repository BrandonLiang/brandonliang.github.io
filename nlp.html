<!DOCTYPE html>
<html>
        <head>
                <title>Xudong Liang (Brandon) - NLP Resource</title>
                <!-- link to main stylesheet -->
                <link rel="stylesheet" type="text/css" href="/css/main.css">
                <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
        </head>
        <body>
                <h1>Xudong Liang (Brandon)</h1>
                <nav>
                <ul>
                        <li><a href="/">Home</a></li>
                        <li><a href="./docs/Xudong Liang (Brandon) Resume.pdf">Resume</a></li>
                        <li><a href="./education.html">Education</a></li>
                        <li><a href="./experience.html">Experience</a></li>
                        <li><a href="./portfolio.html">Portfolio</a></li>
                        <li><a href="./twitter.html">Twitter Analysis</a></li>
                        <li><a href="./nba.html">NBA Analytics</a></li>
                        <li style="color:blue;"><a href="./nlp.html">NLP Resource</a></li>
                        <li><a href="./software_engineering.html">SE Resource</a></li>
                        <li><a href="./business.html">Business Resource</a></li>
                        <li><a href="https://www.linkedin.com/in/xudong-brandon-liang/"><i class="fab fa-linkedin big-icon"></i></a></li>
                        <li><a href="https://github.com/brandonliang"><i class="fab fa-github big-icon"></i></a></li>
                        <li><a href="mailto:xl2891@columbia.edu"><i class="fas fa-envelope big-icon"></i></a></li>
                        <li><a href="https://www.instagram.com/brandonliangtruth/"><i class="fab fa-instagram big-icon"></i></a></li>
                </ul>
                </nav>
                <div class="container">
                <div class="blurb">
                        <h2>NLP Resource</h2>
                        <p>Bayes Theorem, Hidden Markov Models, Conditional Random Field</p>
                        <ul class="container">
                          <li><a href="https://towardsdatascience.com/understand-bayes-rule-likelihood-prior-and-posterior-34eae0f378c5" font size = "35">Understand Bayes Rule, Likelihood, Prior and Posterior</a></li>
                          <li><a href="https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9" font size = "35">MLE, MAP and Bayesian Inference</a></li>
                          <li><a href="https://www.youtube.com/watch?v=kqSzLo9fenk" font size = "35">A Friendly Introduction to Bayes Theorem and Hidden Markov Models</a></li>
                          <li><a href="https://github.com/luisguiserrano/hmm" font size = "35">Simple Implementation of the Viterbi Algorithm for training Hidden Markov Models (Github)</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/58221/intuitive-difference-between-hidden-markov-models-and-conditional-random-fields/342043#342043?newreg=7485904e05574a04adfdcc847cd9b994" font size="35">Intuitive difference between Hidden Markov models and Conditional Random Fields</a></li>
                          <li><a href="http://pages.cs.wisc.edu/~jerryzhu/cs838/CRF.pdf" font size = "35">CS838-1 Advanced NLP: Conditional Random Field</a></li>
                          <li><a href="https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541" font size = "35">Overview of Conditional Random Field</a></li>
                          <li><a href="https://towardsdatascience.com/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea" font size = "35">Implement Conditional Random Field in PyTorch</a></li>
                          <li><a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers" font size = "35">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/282987/hidden-markov-model-vs-recurrent-neural-network/414029#414029" font size = "35">HMM vs RNN</a></li>
                          <li><a href="https://stathwang.github.io/part-of-speech-tagging-with-trigram-hidden-markov-models-and-the-viterbi-algorithm.html" font size = "35">Part-of-Speech Tagging with Trigram Hidden Markov Models and the Viterbi Algorithm</a></li>
                        </ul>
                        <p>Neural Networks</p>
                        <ul class="container">
                          <li><a href="http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/" font size = "35">A Visual and Interactive Guide to the Basics of Neural Networks</a></li>
                          <li><a href="https://jalammar.github.io/feedforward-neural-networks-visual-interactive/" font size = "35">A Visual And Interactive Look at Basic Neural Network Math</a></li>
                        </ul>
                        <p>RNN</p>
                        <ul class="container">
                          <li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" font size = "35">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
                          <li><a href="https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/" font size = "35">LSTM: From Zero to Hero with PyTorch</a></li>
                          <li><a href="https://stackoverflow.com/a/48305882" font size = "35">Difference Between "Output" and "Hidden States (& Cell States)" in LSTM (PyTorch)</a></li>
                          <li><a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" font size = "35">The fall of RNN/LSTM</a></li>
                        </ul>
                        <p>Word Embeddings: Word2Vec - CBOW, Skigram (SGNS, SVD, PPMI, Neural Word Embedding); GloVe</p>
                        <ul class="container">
                          <li><a href="http://jalammar.github.io/illustrated-word2vec/" font size = "35">The Illustrated Word2vec</a></li>
                          <li><a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf" font size = "35">CS224n: Word Vectors: Intro, SVD and Word2Vec</a></li>
                          <li><a href="https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314" font size = "35">Word2Vec - Skip Gram vs CBOW</a></li>
                          <li><a href="https://github.com/stanfordnlp/GloVe" font size = "35">Stanford NLP: GloVe - Global Vectors for Word Representation</a></li>
                          <li><a href="https://github.com/Embedding/word2vec" font size = "35">Word2Vec</a></li>
                          <li><a href="https://github.com/zhezhaoa/ngram2vec" font size = "35">Ngram2Vec</a></li>
                          <li><a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" font size = "35">SVD, PPMI, SGNS, Neural Word Embedding as Implicit Matrix Factorization (Paper)</a></li>
                          <li><a href="http://jalammar.github.io/mit-analytics-lab-talk/" font size = "35">Language Models and Skipgram Recommenders Talk @ MIT</a></li>
                          <li><a href="http://jalammar.github.io/skipgram-recommender-talk/" font size = "35">Skipgram Recommender Talk, Intuition and Use-Cases of Embeddings in NLP & beyond @ Qcon London</a></li>
                          <li><a href="https://www.youtube.com/watch?v=4-QoMdSqG_I" font size = "35">Intuition & Use-Cases of Embeddings in NLP & beyond - YouTube</a></li>
                          <li><a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/nlp/01_Exploring_Word_Embeddings.ipynb" font size = "35">(Notebook) 01 - Exploring Word Embeddings (Gensim)</a></li>
                          <li><a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/nlp/02_Song_Embeddings.ipynb" font size = "35">(Notebook) 02 - Song Embeddings - Skipgram Recommender (Gensim)</a></li>
                          <li><a href="https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb" font size = "35">(Paper) Real-time Personalization using Embeddings for Search Ranking at Airbnb</a></li>
                          <li><a href="https://arxiv.org/abs/1803.02349" font size = "35">(Paper) Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</a></li>
                          <li> - Word2Vec: use words to predict words</li>
                          <li> -- CBOW: predict the word in the middle</li>
                          <li> -- Skip-gram (SGNS): predict the context window (predict neighboring word)</li>
                          <li> - fastText</li>
                          <li> -- Same goal as Word2Vec: learn vector rep of words</li>
                          <li> -- But, uses character n-grams and character vector to predict words</li>
                          <li> -- Words are represented by the sum of character n-gram vectors</li>
                          <li> - GloVe</li>
                          <li> -- Global word-word co-occurrence and matrix factorization for word embedding on word-context matrix</li>
                           <li> -- PPMI</li>
                        </ul>
                        <p>Transformers, Attention, Self-Attention</p>
                        <ul class="container">
                          <li><a href="https://huggingface.co/docs/transformers/tokenizer_summary" font size = "35">Transformer Tokenizer Summary</a></li>
                          <li><a href="https://medium.com/@joealato/attention-in-nlp-734c6fa9d983" font size = "35">Attention</a></li>
                          <li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" font size = "35">Attention, Self-Attention, Families of Attention, and more</a></li>
                          <li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" font size = "35">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li>
                          <li><a href="https://github.com/tensorflow/nmt" font size = "35">Neural Machine Translation (seq2seq) Tutorial With Attention (Tensorflow Github)</a></li>
                          <li><a href="https://arxiv.org/abs/1706.03762" font size = "35">(Paper) Attention Is All You Need - proposed Transformer</a></li>
                          <li><a href="http://jalammar.github.io/illustrated-transformer/" font size = "35"> The Illustrated Transformer (with Self-Attention)</a></li>
                          <li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" font size = "35">The Annotated Transformer - "Attention is all you need" - Harvard NLP</a></li>
                          <li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" font size = "35">Google AI Blog - Transformer: A Novel Neural Network Architecture for Language Understanding</a></li>
                          <li><a href="http://jalammar.github.io/illustrated-bert/" font size = "35">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></li>
                          <li><a href="https://nlp.stanford.edu/seminar/details/jdevlin.pdf" font size = "35">BERT - Slides - By Jacob Devlin - Google AI Language</a></li>
                          <li><a href="https://huggingface.co/transformers/_modules/transformers/modeling_bert.html" font size = "35">Source code for transformers.modeling_bert (Hugging Face)</a></li>
                          <li><a href="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial" font size = "35">BERT Word Embeddings Tutorial</a></li>
                          <li><a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" font size = "35">A Visual Guide to Using BERT for the First Time (Hugging Face)</a></li>
                          <li><a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/nlp/03_Sentence_Classification_with_BERT.ipynb" font size = "35">(Notebook) 03 - Sentence Classification with BERT</a></li>
                          <li><a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/Attention_Flow_Graph_Tools.ipynb" font size = "35">(Notebook) Attention Flow Graph Tools</a></li>
                          <li><a href="https://huggingface.co/blog/how-to-train" font size = "35">(Hugging Face Tutorial) How to train a new language model from scratch using Transformers and Tokenizers</a></li>
                          <li><a href="https://huggingface.co/transformers/main_classes/optimizer_schedules.html" font size = "35">Hugging Face Transformers Optimizer_Schedules (e.g.: transformers.get_linear_schedule_with_warmup)</a></li>
                          <li><a href="https://huggingface.co/" font size = "35">Hugging Face</a></li>
                          <li><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" font size = "35">PyTorch: Translation With A Sequence To Sequence Network And Attention</a></li>
                          <li><a href="http://jalammar.github.io/explaining-transformers/" font size = "35">Interfaces for Explaining Transformer Language Models</a></li>
                          <li><a href="http://jalammar.github.io/hidden-states/" font size = "35">Finding the Words to Say: Hidden State Visualizations for Language Models</a></li>
                          <li><a href="https://paperswithcode.com/method/bart#:~:text=BART%20is%20a%20denoising%20autoencoder,based%20neural%20machine%20translation%20architecture." font size = "35">BART Explained | Papers With Code</a></li>
                          <li><a href="https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model" font size = "35">Computational Complexity of Self-Attention in the Transformer Model (vs. RNN)</a></li>
                        </ul>
                        <p>LLM, GPT & Few-Shot & Zero-Shot Learning</p>
                        <ul class="container">
                          <li><a href="https://leimao.github.io/blog/Byte-Pair-Encoding/" font size = "35">Byte Pair Encoding</a></li>
                          <li><a href="https://paperswithcode.com/method/gpt" font size = "35">GPT Explained | Papers With Code</a></li>
                          <li><a href="https://paperswithcode.com/method/gpt-2" font size = "35">GPT-2 Explained | Papers With Code</a></li>
                          <li><a href="http://jalammar.github.io/illustrated-gpt2/" font size = "35">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></li>
                          <li><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html" font size = "35">The Annotated GPT-2</a></li>
                          <li><a href="https://huggingface.co/transformers/_modules/transformers/modeling_gpt2.html" font size = "35">Source code for transformers.modeling_gpt2 (Hugging Face)</a></li>
                          <li><a href="https://huggingface.co/transformers/model_doc/gpt2.html#gpt2config" font size = "35">Hugging Face GPT2Config</a></li>
                          <li><a href="https://arxiv.org/abs/2101.06804" font size = "35">What Makes Good In-Context Examples for GPT-3?</a></li>
                          <li><a href="https://medium.com/swlh/learning-to-write-language-generation-with-gpt-2-2a13fa249024" font size = "35">Learning to Write: Language Generation With GPT-2</a></li>
                          <li><a href="http://jalammar.github.io/illustrated-transformer/" font size = "35">AllenAI GPT-2 Explorer</a></li>
                          <li><a href="https://arxiv.org/abs/2109.01652" font size = "35">Finetuned Language Models Are Zero-Shot Learners</a></li>
                          <li><a href="https://arxiv.org/abs/2012.15723" font size = "35">Making Pre-trained Language Models Better Few-Shot Learners</a></li>
                          <li><a href="https://arxiv.org/abs/2104.08691" font size = "35">The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
                          <li><a href="https://arxiv.org/abs/2101.00190" font size = "35">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></li>
                          <li><a href="https://paperswithcode.com/method/pattern-exploiting-training" font size = "35">Pattern-Exploiting Training</a></li>
                          <li><a href="https://arxiv.org/abs/2112.08348" font size = "35">PROMPT WAYWARDNESS: The Curious Case of Discretized Interpretation of Continuous Prompts (investigates what prompts are actually learning)</a></li>
                          <li><a href="https://arxiv.org/abs/2106.08367" font size = "35">What Context Features Can Transformer Language Models Use (investigates what information is acutally useful from very long contexts)</a></li>
                          <li><a href="https://arxiv.org/abs/2001.07676" font size = "35">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</a></li>
                          <li><a href="https://arxiv.org/abs/2104.08821" font size = "35">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a></li>
                          <li><a href="https://arxiv.org/abs/2204.05832" font size = "35">What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</a></li>
                          <li><a href="https://arxiv.org/pdf/2102.09690.pdf" font size = "35">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a></li>
                          <li><a href="https://towardsdatascience.com/perplexity-in-language-models-87a196019a94" font size = "35">Perplexity in Language Models</a></li>
                          <li><a href="https://jaketae.github.io/study/gpt2/#setup" font size = "35">NLG with GPT-2 - Jake Tae</a></li>
                          <li><a href="https://huggingface.co/blog/how-to-generate" font size = "35">HuggingFace: How to generate text: using different decoding methods for language generation with Transformers</a></li>
                          <li><a href="https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models" font size = "35">New Scaling Laws for Large Language Models</a></li>
                          <li><a href="https://github.com/huggingface/transformers/issues/18388" font size = "35">Mismatch between logits from generate and forward with an attention mask for most GPT models</a></li>
                          <li><a href="https://stackoverflow.com/questions/73411215/getting-logits-from-t5-hugging-face-model-using-forward-method-without-labels" font size = "35">Getting logits from T5 Hugging Face model using forward() method without labels</a></li>
                          <li><a href="https://stackoverflow.com/questions/67328345/how-to-use-forward-method-instead-of-model-generate-for-t5-model" font size = "35">How to use forward() method instead of model.generate() for T5 model -- you should not use forward() for generation, why?</a></li>
                          <li><a href="https://snorkel.ai/better-not-bigger-how-to-get-gpt-3-quality-at-0-1-the-cost/" font size = "35">Better not bigger: How to get GPT-3 quality at 0.1% the cost</a></li>
                          <li><a href="https://openai.com/blog/instruction-following/" font size = "35">InstructGPT: Aligning Language Models to Follow Instructions</a></li>
                          <li><a href="https://github.com/openai/following-instructions-human-feedback/blob/main/model-card.md" font size = "35">InstructGPT Model Card</a></li>
                          <li><a href="https://arxiv.org/abs/2203.02155" font size = "35">Training language models to follow instructions with human feedback</a></li>
                          <li><a href="https://arxiv.org/abs/2201.11903" font size = "35">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></li>
                          <li><a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html" font size = "35">Language Models Perform Reasoning via Chain of Thought</a></li>
                          <li><a href="https://openai.com/blog/chatgpt/" font size = "35">ChatGPT: Optimizing Language Models for Dialogue</a></li>
                          <li><a href="https://openai.com/blog/language-model-safety-and-misuse/" font size = "35">Lessons Learned on Language Model Safety and Misuse</a></li>
                          <li><a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models" font size = "35">Understanding Large Language Models - A Cross-Section of the Most Relevant Literature to Get Up To Speed</a></li>
                          <li><a href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models" font size = "35">Finetuning Large Language Models - an Introduction to the core ideas and appraoches - in-context learning and finetuning</a></li>
                          <li><a href="https://www.promptingguide.ai/" font size = "35">Prompt Engineering Guide</a></li>
                          <li><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/" font size = "35">Prompt Engineering</a></li>
                          <li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/" font size = "35">Large Tnrasformer Model Inference Optimization</a></li>
                          <li><a href="https://huyenchip.com/2023/04/11/llm-engineering.html" font size = "35">Building LLM applications for production</a></li>
                        </ul>
                        <p>Encoder, Decoder, Autoencoder, Decoding, Sampling, Attention</p>
                        <ul class="container">
                          <li><a href="https://datascience.stackexchange.com/a/65242" font size = "35">Difference between Transformer Encoder, Decoder and Encoder-Decoder architectures</a></li>
                          <li><a href="https://ai.stackexchange.com/questions/4245/what-is-the-difference-between-encoders-and-auto-encoders#:~:text=To%20answer%20this%20rather%20succinctly,is%20what%20the%20brain%20does.&text=An%20autoencoder%20will%20have%20the,to%20predict%20the%20Y%20target." font size = "35">What is the difference between encoders and auto-encoders?</a></li>
                          <li><a href="https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277" font size = "35">How to sample from language models - Temprature and Top k Sampling</a></li>
                          <li><a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" font size = "35">Seq2Seq with Attention and Beam Search</a></li>
                          <li><a href="https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/#:~:text=Attention%20is%20proposed%20as%20a,problem%20when%20decoding%20long%20sequences." font size = "35">How does Attention work in Encoder-Decoder Recurrent Neural Networks</a></li>
                          <li><a href="https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7" font size = "35">Beam Search Decoding in CTC-trained Neural Networks</a></li>
                          <li><a href="https://www.jeremyjordan.me/autoencoders/" font size = "35">Introduction to Autoencoders</a></li>
                          <li><a href="https://www.jeremyjordan.me/variational-autoencoders/" font size = "35">Introduction to Variational Autoencoders</a></li>
                          <li><a href="https://ark2210.medium.com/learning-disentangled-representations-with-variational-autoencoders-b1bfe237fffb" font size = "35">Learning Disentangled Representations with Variational Autoencoders</a></li>
                          <li><a href="https://github.com/facebookresearch/UnsupervisedMT" font size = "35">Unsupervised Machine Translation (Github)</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/120080/whatre-the-differences-between-pca-and-autoencoder#:~:text=As%20bayerj%20points%20out%20PCA,can%20yield%20the%20same%20result." font size = "35">Difference between PCA and Autoencoder</a></li>
                          <li><a href="https://towardsdatascience.com/autoencoders-vs-pca-when-to-use-which-73de063f5d7#:~:text=PCA%20is%20essentially%20a%20linear,modelling%20complex%20non%20linear%20functions.&text=PCA%20is%20faster%20and%20computationally,is%20very%20similar%20to%20PCA." font size = "35">Autoencoder vs PCA: when to use?</a></li>
                          <li><a href="https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc" font size = "35">Decoding Strategies that You Need to Know for Response Generation - Distinguish between Beam Search, Random Sampling, Top-K, and Nucleus</a></li>
                          <li><a href="https://www.quora.com/Whats-the-difference-between-a-Variational-Autoencoder-VAE-and-an-Autoencoder" font size = "35">What's the difference between a Variational Autoencoder (VAE) and an Autoencoder?</a></li>
                          <li><a href="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html#:~:text=Bert%20vs.,-GPT2&text=As%20the%20BART%20authors%20write,to%20make%20a%20good%20guess." font size = "35">Introducing BART (vs. BERT & GPT2)</a></li>
                        </ul>
                        <p>Loss Function</p>
                        <ul class="container">
                          <li><a href="https://stats.stackexchange.com/a/357974/301274" font size = "35">What is the difference Cross-entropy and KL divergence?</a></li>
                          <li><a href="https://leimao.github.io/blog/Cross-Entropy-KL-Divergence-MLE/" font size = "35">(Proof) Cross Entropy, KL Divergence and Maximum Likelihood Estimation</a></li>
                          <li><a href="https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816/4" font size = "35">Difference between Cross-Entropy Loss and (Negative) Log Likelihood Loss (PyTorch)</a></li>
                          <li><a href="https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/other/pytorch-lossfunc-cheatsheet.md" font size = "35">PyTorch Loss-Input Confusion (Cheatsheet) - Cross_Entropy = Log_Softmax + NLL_Loss</a></li>
                          <li><a href="https://ai.stackexchange.com/questions/12068/whats-the-advantage-of-log-softmax-over-softmax" font size = "35">Advantage of log_softmax over softmax (PyTorch)</a></li>
                          <li><a href="https://amaarora.github.io/2020/07/18/label-smoothing.html" font size = "35">Label Smoothing as Regularization</a></li>
                          <li><a href="https://distill.pub/2017/ctc/" font size = "35">Sequence Modeling with Connectionist Temporal Classification (CTC) Loss</a></li>
                          <li><a href="https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/" font size = "35">KL Divergence: Forward vs. Reverse?</a></li>
                        </ul>
                        <p>Machine Learning, Linear Algebra</p>
                        <ul class="container">
                          <li><a href="https://souravsengupta.com/cds2016/lectures/Savov_Notes.pdf" font size = "35">Linear Algebra exaplined in 4 pages</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/72087/r-squared-vs-t-test-confusion" font size = "35">R-squared vs. t-test confusion</a></li>
                          <li><a href="http://www.cs.columbia.edu/~verma/classes/ml/index.html" font size = "35">COMS 4771 Machine Learning</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves/7210#7210" font size = "35">ROC vs precision-recall curvers</a></li>
                          <li><a href="https://dustinstansbury.github.io/theclevermachine/bias-variance-tradeoff" font size = "35">Bias-Variance Tradeoff (Noise)</a></li>
                          <li><a href="http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/" font size = "35">Bias-Variance Decomposition - mlxtend</a></li>
                          <li><a href="https://towardsdatascience.com/introduction-to-optimization-with-genetic-algorithm-2f5001d9964b" font size = "35">Introduction to Optimization with Genetic Algorithm</a></li>
                          <li><a href="https://blog.zakjost.com/post/nce-intro/" font size = "35">Intro to Noise Contrastic Estimation</a></li>
                          <li><a href="https://www.eecs189.org/static/notes/n26.pdf" font size = "35">CS189 - Boosting</a></li>
                          <li><a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" font size = "35">A Gentle Intro to Gradient Boosting Algorithm</a></li>
                          <li><a href="https://machinelearningmastery.com/difference-test-validation-datasets/" font size = "35">Difference Between Test and Validation Datasets</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution" font size = "35">What is the intuition behind beta distribution</a></li>
                          <li><a href="https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af#:~:text=The%20difference%20between%20the%20binomial,probability%20is%20a%20random%20variable." font size = "35">Beta Distribution — Intuition, Examples, and Derivation</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/552778/is-k-means-a-generative-model-and-how-could-it-be-used-to-generate-new-data-then" font size = "35">Is k-means a generative model and how could it be used to generate new data then?</a></li>
                          <li><a href="https://www.aaai.org/Papers/ICML/2003/ICML03-110.pdf" font size = "35">Optimizing Classifier Performance (AUC-ROC) via an Approximation to the Wilcoxon-Mann-Whitney (WMW) Statistic</a></li>
                          <li><a href="https://towardsdatascience.com/automl-for-fast-hyperparameters-tuning-with-smac-4d70b1399ce6" font size = "35">SMAC: AutoML for fast Hyperparameters tuning in Python</a></li>
                        </ul>
                        <p>Machine Learning Operations - MLOps</p>
                        <ul class="container">
                          <li><a href="https://arxiv.org/pdf/2209.09125.pdf" font size = "35">Operationalizing Machine Learning: An Interview Study</a></li>
                        </ul>
                        <p>Deep Learning</p>
                        <ul class="container">
                          <li><a href="https://towardsdatascience.com/full-implementation-of-gradient-descent-no-holding-back-86e688f36064" font size = "35">Full Implementation of Gradient Descent - no holding back</a></li>
                          <li><a href="https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404" font size = "35">Weight Initializers (Random, Normal, Xavier, He)</a></li>
                          <li><a href="https://ruder.io/optimizing-gradient-descent/index.html" font size = "35">An Overview of Gradient Descent Optimization Algorithms</a></li>
                          <li><a href="https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html" font size = "35">Neural Architecture Search</a></li>
                          <li><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" font size = "35">A Step by Step Backpropagation Example</a></li>
                          <li><a href="Incorporating Nesterov Momentum into Adam" font size = "35">Incorporating Nesterov Momentum into Adam</a></li>
                          <li><a href="https://towardsdatascience.com/batch-normalization-the-greatest-breakthrough-in-deep-learning-77e64909d81d" font size = "35">Batch Normalization: The Greatest Breakthrough in Deep Learning</a></li>
                          <li><a href="https://www.quora.com/How-does-Batch-Normalization-battle-the-fact-that-gradients-might-explode-when-training-a-Neural-Network" font size = "35">How does Batch Normalization battle the fact that gradients might explode when training a Neural Network?</a></li>
                          <li><a href="https://atcold.github.io/pytorch-Deep-Learning/" font size = "35">Deep Learning with PyTorch - Open Source Course by Yann LeCun and NYU</a></li>
                          <li><a href="https://homes.cs.washington.edu/~marcotcr/blog/lime/" font size = "35">LIME - Local Interpretable Model-Agnostic Explanations</a></li>
                          <li><a href="https://sassafras13.github.io/GumbelSoftmax/" font size = "35">The Gumbel-Softmax Distribution</a></li>
                          <li><a href="https://arxiv.org/pdf/2111.09832.pdf" font size = "35">Merging Models with Fisher-Weighted Averaging</a></li>
                          <li><a href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f" font size = "35">A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning</a></li>
                          <li><a href="https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem" font size = "35">Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem)</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/535802/upsampling-vs-class-weights-in-mini-batch-sgd" font size = "35">upsampling vs class weights in mini-batch SGD for class imbalance (and why you should favor upsampling)</a></li>
                        </ul>
                        <p>Reinforcement Learning</p>
                        <ul class="container">
                          <li><a href="https://openai.com/blog/openai-baselines-ppo/" font size = "35">Proximal Policy Optimization</a></li>
                        </ul>
                        <p>Miscellaneous - NLP</p>
                        <ul class="container">
                          <li><a href="https://monkeylearn.com/blog/introduction-to-topic-modeling/#what-is-topic-modeling" font size = "35">Topic Modeling</a></li>
                          <li><a href="https://github.com/SophonPlus/ChineseNlpCorpus" font size = "35">Chinese NLP Corpus</a></li>
                          <li><a href="https://aideadlin.es/?sub=ML,NLP" font size = "35">NLP & ML Conference Deadlines</a></li>
                          <li><a href="https://towardsdatascience.com/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc" font size = "35">The relationship between Perplexity and Entropy in NLP</a></li>
                          <li><a href="https://stackabuse.com/python-for-nlp-topic-modeling/" font size = "35">Python for NLP Topic Modeling</a></li>
                          <li><a href="https://github.com/joewandy/hlda" font size = "35">Hierarchical Latent Dirichlet Allocation (hLDA) for Topic Modeling - Github</a></li>
                          <li><a href="https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/" font size = "35">Topic Modeling Gensim Python</a></li>
                          <li><a href="https://github.com/niderhoff/nlp-datasets" font size = "35">Free NLP Datasets</a></li>
                          <li><a href="https://albertauyeung.github.io/2018/06/03/generating-ngrams.html" font size = "35">Generating N-grams from Sentences in Python</a></li>
                          <li><a href="https://github.com/jalammar/ecco/" font size = "35">(Github) Ecco - python library for explaining NLP models using interactive visualizations</a></li>
                          <li><a href="https://dlab.epfl.ch/people/west/pub/Pavllo-Piccardi-West_ICWSM-18.pdf" font size = "35">Quootstrap: Scalable Unsupervised Extraction of QuotationSpeaker Pairs from Large News Corpora via Bootstrapping</a></li>
                          <li><a href="https://dl.acm.org/doi/pdf/10.1145/3437963.3441760?casa_token=fJQJlb4UheUAAAAA%3ArwlK1_RU-JRnn0tWdB22Qw51pFqCREstaC6R1CQ3l_9NTBMjETpMwBvmK6itWVmAfRxBMuqp0PYGZA" font size = "35">Quotebank: A Corpus of Quotations from a Decade of News (& Quobert)</a></li>
                        </ul>
                        <p>Data Sampling, Augmentation, Active Learning, Active Testing</p>
                        <ul class="container">
                          <li><a href="https://arxiv.org/abs/1901.11196" font size = "35">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</a></li>
                          <li><a href="https://burrsettles.com/pub/settles.activelearning.pdf" font size = "35">Active Learning Literature Survey</a></li>
                          <li><a href="https://aclanthology.org/2020.emnlp-main.638.pdf" font size = "35">Active Learning for BERT: An Empirical Study</a></li>
                          <li><a href="https://aclanthology.org/2021.emnlp-main.51.pdf" font size = "35">Active Learning by Acquiring Contrastive Examples</a></li>
                          <li><a href="https://arxiv.org/abs/2104.08320" font size = "35">On the Importance of Effectively Adapting Pretrained Language models for Active Learning</a></li>
                          <li><a href="https://arxiv.org/pdf/2103.05331.pdf" font size = "35">Active Testing: Sample-Efficient Model Evaluation</a></li>
                        </ul>
                        <p>Open Source Implementations - NLP</p>
                        <ul class="container">
                          <li><a href="https://github.com/Embedding/Chinese-Word-Vectors" font size = "35">Chinese Word Vectors</a></li>
                          <li><a href="https://github.com/OpenNMT/OpenNMT-py" font size = "35">OpenNMT-py: Open-Source Neural Machine Translation in PyTorch</a></li>
                          <li><a href="https://github.com/facebookresearch/UnsupervisedMT" font size = "35">Unsupervised Machine Translation</a></li>
                          <li><a href="https://github.com/facebookresearch/fastText" font size = "35">Facebook Research: fastText</a></li>
                          <li><a href="https://github.com/victoresque/pytorch-template" font size = "35">PyTorch Template Project</a></li>
                          <li><a href="http://www.cleverhans.io/" font size = "35">Cleverhans Blog - Library for Benchmarking the Vulnerability of Machine Learning models to Adversarial Examples</a></li>
                          <li><a href="https://github.com/interpretml/interpret-text" font size = "35">Interpret-Text - Github</a></li>
                          <li><a href="https://github.com/lilianweng/stock-rnn" font size = "35">Predict stock market prices using RNN</a></li>
                          <li><a href="https://github.com/justmarkham/scikit-learn-tips" font size = "35">Daily Scikit-Learn Tips</a></li>
                        </ul>
                        <p>Logistics: Conda, Python, Tensorboard, Jupyter</p>
                        <ul class="container">
                          <li><a href="https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf" font size = "35">Conda Cheatsheet</a></li>
                          <li><a href="https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514" font size = "35">Tensorboard Logging</a></li>
                          <li><a href="https://stackoverflow.com/a/55281501" font size = "35">How to accumulate gradients for large batch sizes in Keras</a></li>
                          <li><a href="https://medium.com/google-cloud/running-jupyter-notebooks-on-gpu-on-google-cloud-d44f57d22dbd" font size = "35">Running Jupyter Notebooks on GPU on Google Cloud</a></li>
                          <li><a href="https://github.com/naming-convention/naming-convention-guides/tree/master/python" font size = "35">Python Naming Convention Guides</a></li>
                          <li><a href="https://realpython.com/python-kwargs-and-args/" font size = "35">Python args and kwargs</a></li>
                          <li><a href="https://realpython.com/primer-on-python-decorators/#simple-decorators" font size = "35">Primer on Python Decorators</a></li>
                          <li><a href="https://www.pythonlikeyoumeanit.com/Module2_EssentialsOfPython/Iterables.html" font size = "35">Python Iterables</a></li>
                          <li><a href="https://www.w3schools.com/python/python_iterators.asp" font size = "35">Python Iterators (W3 Schools)</a></li>
                          <li><a href="https://lucene.apache.org/solr/guide/8_7/solr-tutorial.html" font size = "35">Apache Solr Tutorial</a></li>
                          <li><a href="https://github.com/PyFPDF/fpdf2" font size = "35">fpdf2 - PDF for Python</a></li>
                          <li><a href="https://jekyllrb.com/" font size = "35">Jekyll: Transform your plain text into static websites and blogs</a></li>
                          <li><a href="https://github.com/barryclark/jekyll-now" font size = "35">Jekyll-now (Github)</a></li>
                        </ul>
                        <p>Papers</p>
                        <ul class="container">
                          <li><a href="https://github.com/dair-ai/nlp_paper_summaries" font size = "35">NLP Paper Summaries (Github)</a></li>
                          <li><a href="https://arxiv.org/abs/2004.04906" font size = "35">Dense Passage Retrieval for Open-Domain Question Answering</a></li>
                          <li><a href="https://arxiv.org/abs/1711.00043" font size = "35">Unsupervised Machine Translation Using Monolingual Corpora Only</a> <a href="https://github.com/facebookresearch/UnsupervisedMT" font size = "35">(Github)</a></li>
                          <li><a href="https://www.aclweb.org/anthology/2020.acl-demos.40.pdf" font size = "35">NSTM: Real-Time Query-Driven News Overview Composition at Bloomberg</a></li>
                          <li><a href="https://arxiv.org/abs/1907.03040" font size = "35">BERT-DST: Scalable E2E Dialogue State Tracking with BERT</a></li>
                          <li><a href="https://github.com/tomohideshibata/BERT-related-papers" font size = "35">BERT-Related papers</a></li>
                          <li><a href="https://www.arxiv-vanity.com/papers/1906.05743/" font size = "35">Contrastive Bidirectional Transformer for Temporal Representation Learning</a></li>
                          <li><a href="https://arxiv.org/abs/1810.09587" font size = "35">StateNet: Towards Universal Dialogue State Tracking</a></li>
                          <li><a href="https://arxiv.org/abs/1908.01946" font size = "35">Dialog State Tracking: A Neural Reading Comprehension Approach</a></li>
                          <li><a href="https://arxiv.org/pdf/1911.04252.pdf" font size = "35">Self-training with Noisy Student improves ImageNet classification</a></li>
                          <li><a href="https://arxiv.org/pdf/1607.02533.pdf" font size = "35">Adversarial Examples In The Physical World</a></li>
                          <li><a href="https://arxiv.org/pdf/1708.04552.pdf" font size = "35">Improved Regularization of Convolutional Neural Networks with Cutout</a></li>
                          <li><a href="https://paperswithcode.com/task/dialogue-state-tracking" font size = "35">Dialgoue State Tracking SOTA</a></li>
                          <li><a href="https://arxiv.org/abs/1607.06520" font size = "35">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a> <a href="https://github.com/tolga-b/debiaswe" font size = "35">(Github)</a></li>
                          <li><a href="https://github.com/eugeneyan/applied-ml" font size = "35">Applied-ML (Github): Curated papers, articles, and blogs on data science & machine learning in production.</a></li>
                          <li><a href="https://www.paperdigest.org/2021/11/neurips-2021-highlights/" font size = "35">Paper Digest: NeurIPS 2021 Highlights</a></li>
                          <li><a href="https://arxiv.org/abs/2210.11416" font size = "35">Scaling Instruction-Finetuned Language Models</a></li>
                        </ul>
                        <p>CNN, CV</p>
                        <ul class="container">
                          <li><a href="https://cs231n.github.io/convolutional-networks/" font size = "35">CS231n Convolutional Neural Networks for Visual Recognition</a></li>
                          <li><a href="https://deeplizard.com/learn/video/0LhiS6yu2qQ" font size = "35">CNN Confusion Matrix With PyTorch - Neural Network Programming</a></li>
                          <li><a href="https://github.com/kubeflow-kale/examples/blob/master/pytorch-classification/cifar10_classification.ipynb" font size = "35">Cifar10 Classification Jupyter Notebook</a></li>
                          <li><a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py" font size = "35">PyTorch: Resnet</a></li>
                          <li><a href="https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py" font size = "35">Keras: Resnet on Cifar10</a></li>
                          <li><a href="https://github.com/uzaymacar/self-supervision" font size = "35">Self-Supervised Learning for Computer Vision (Github)</a></li>
                        </ul>
                        <p>Speech and ASR</p>
                        <ul class="container">
                          <li><a href="https://neuron.eng.wayne.edu/tarek/MITbook/chap5/5_4.html" font size = "35">Time-Delay Neural Network</a></li>
                          <li><a href="https://github.com/iamyuanchung/speech2vec-pretrained-vectors" font size = "35">Speech2Vec Pretrained Vectors (Github)</a></li>
                          <li><a href="https://verbyx.com/2020/01/13/understanding-speech-recognition/" font size = "35">Understanding Speech Recognition</a></li>
                          <li><a href="https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6#:~:text=Kaldi%20is%20an%20open%20source,speaker%20recognition%20and%20speaker%20diarisation.&text=Kaldi%20is%20written%20mainly%20in,with%20Bash%20and%20Python%20scripts." font size = "35">How to start with Kaldi and Speech Recognition</a></li>
                          <li><a href="http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" font size = "35">Mel Frequency Cepstral Coefficient (MFCC) Tutorial</a></li>
                          <li><a href="http://kaldi-asr.org/doc/data_prep.html" font size = "35">Kaldi Data Preparation</a></li>
                          <li><a href="http://kaldi-asr.org/doc/kaldi_for_dummies.html" font size = "35">Kaldi for Dummies Tutorial</a></li>
                          <li><a href="https://labrosa.ee.columbia.edu/matlab/rastamat/" font size = "35">MFCC in Matlab</a></li>
                          <li><a href="https://github.com/jameslyons/python_speech_features" font size = "35">MFCC in Python (Github)</a> - <a href="https://python-speech-features.readthedocs.io/en/latest/" font size = "35">Documentation</a></li>
                          <li><a href="https://github.com/YoavRamon/awesome-kaldi" font size = "35">Awesome Kaldi (Github)</a></li>
                          <li><a href="https://www.eleanorchodroff.com/tutorial/kaldi/" font size = "35">Great Kaldi Tutorial</a></li>
                        </ul>
        </body>
</html>
