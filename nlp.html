<!DOCTYPE html>
<html>
        <head>
                <title>Xudong Liang (Brandon) - NLP Resource</title>
                <!-- link to main stylesheet -->
                <link rel="stylesheet" type="text/css" href="/css/main.css">
                <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
        </head>
        <body>
                <h1>Xudong Liang (Brandon)</h1>
                <nav>
                <ul>
                        <li><a href="/">Home</a></li>
                        <li><a href="./docs/Xudong Liang (Brandon) Resume.pdf">Resume</a></li>
                        <li><a href="./education.html">Education</a></li>
                        <li><a href="./experience.html">Experience</a></li>
                        <li><a href="./portfolio.html">Portfolio</a></li>
                        <li><a href="./twitter.html">Twitter Analysis</a></li>
                        <li><a href="./nba.html">NBA Analytics</a></li>
                        <li style="color:blue;"><a href="./nlp.html">NLP Resource</a></li>
                        <li><a href="https://www.linkedin.com/in/xudong-brandon-liang/"><i class="fab fa-linkedin big-icon"></i></a></li>
                        <li><a href="https://github.com/brandonliang"><i class="fab fa-github big-icon"></i></a></li>
                        <li><a href="mailto:xl2891@columbia.edu"><i class="fas fa-envelope big-icon"></i></a></li>
                        <li><a href="https://www.instagram.com/brandonliangtruth/"><i class="fab fa-instagram big-icon"></i></a></li>
                </ul>
                </nav>
                <div class="container">
                <div class="blurb">
                        <h2>NLP Resource</h2>
                        <p>Bayes Theorem, Hidden Markov Models, Conditional Random Field</p>
                        <ul class="container">
                          <li><a href="https://www.youtube.com/watch?v=kqSzLo9fenk" font size = "35">A Friendly Introduction to Bayes Theorem and Hidden Markov Models</a></li>
                          <li><a href="https://github.com/luisguiserrano/hmm" font size = "35">Simple Implementation of the Viterbi Algorithm for training Hidden Markov Models (Github)</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/58221/intuitive-difference-between-hidden-markov-models-and-conditional-random-fields/342043#342043?newreg=7485904e05574a04adfdcc847cd9b994" font size="35">Intuitive difference between Hidden Markov models and Conditional Random Fields</a></li>
                          <li><a href="https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541" font size = "35">Overview of Conditional Random Field</a></li>
                          <li><a href="https://towardsdatascience.com/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea" font size = "35">Implement Conditional Random Field in PyTorch</a></li>
                          <li><a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers" font size = "35">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/282987/hidden-markov-model-vs-recurrent-neural-network/414029#414029" font size = "35">HMM vs RNN</a></li>
                        </ul>
                        <p>RNN</p>
                        <ul class="container">
                          <li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" font size = "35">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
                          <li><a href="https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/" font size = "35">LSTM: From Zero to Hero with PyTorch</a></li>
                          <li><a href="https://stackoverflow.com/a/48305882" font size = "35">Difference Between "Output" and "Hidden States (& Cell States)" in LSTM</a></li>
                        </ul>
                        <p>Word Embeddings: Word2Vec, GloVe, SGNS, CBOW, SVD, PPMI, Neural Word Embedding</p>
                        <ul class="container">
                          <li><a href="http://jalammar.github.io/illustrated-word2vec/" font size = "35">The Illustrated Word2vec</a></li>
                          <li><a href="https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314" font size = "35">Word2Vec - Skip Gram vs CBOW</a></li>
                          <li><a href="https://github.com/stanfordnlp/GloVe" font size = "35">Stanford NLP: GloVe - Global Vectors for Word Representation</a></li>
                          <li><a href="https://github.com/Embedding/word2vec" font size = "35">Word2Vec</a></li>
                          <li><a href="https://github.com/zhezhaoa/ngram2vec" font size = "35">Ngram2Vec</a></li>
                          <li><a href="http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" font size = "35">SVD, PPMI, SGNS, Neural Word Embedding as Implicit Matrix Factorization (Paper)</a></li>
                        </ul>
                        <p>Transformers, Attention, Self-Attention</p>
                        <ul class="container">
                          <li><a href="https://medium.com/@joealato/attention-in-nlp-734c6fa9d983" font size = "35">Attention</a></li>
                          <li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" font size = "35">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li>
                          <li><a href="https://github.com/tensorflow/nmt" font size = "35">Neural Machine Translation (seq2seq) Tutorial With Attention (Tensorflow Github)</a></li>
                          <li><a href="http://jalammar.github.io/illustrated-transformer/" font size = "35"> The Illustrated Transformer (with Self-Attention)</a></li>
                          <li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" font size = "35">The Annotated Transformer - "Attention is all you need" - Harvard NLP</a></li>
                          <li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" font size = "35">Google AI Blog - Transformer: A Novel Neural Network Architecture for Language Understanding</a></li>
                          <li><a href="http://jalammar.github.io/illustrated-bert/" font size = "35">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></li>
                          <li><a href="https://nlp.stanford.edu/seminar/details/jdevlin.pdf" font size = "35">BERT - Slides - By Jacob Devlin - Google AI Language</a></li>
                          <li><a href="https://huggingface.co/transformers/_modules/transformers/modeling_bert.html" font size = "35">Source code for transformers.modeling_bert (HuggingFace)</a></li>
                          <li><a href="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial" font size = "35">BERT Word Embeddings Tutorial</a></li>
                          <li><a href="https://leimao.github.io/blog/Byte-Pair-Encoding/" font size = "35">Byte Pair Encoding</a></li>
                          <li><a href="http://jalammar.github.io/illustrated-gpt2/" font size = "35">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></li>
                          <li><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html" font size = "35">The Annotated GPT-2</a></li>
                          <li><a href="https://huggingface.co/transformers/_modules/transformers/modeling_gpt2.html" font size = "35">Source code for transformers.modeling_gpt2 (HuggingFace)</a></li>
                          <li><a href="https://medium.com/swlh/learning-to-write-language-generation-with-gpt-2-2a13fa249024" font size = "35">Learning to Write: Language Generation With GPT-2</a></li>
                          <li><a href="http://jalammar.github.io/illustrated-transformer/" font size = "35">AllenAI GPT-2 Explorer</a></li>
                          <li><a href="https://huggingface.co/" font size = "35">Hugging Face</a></li>
                          <li><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" font size = "35">PyTorch: Translation With A Sequence To Sequence Network And Attention</a></li>
                        </ul>
                        <p>Loss Function</p>
                        <ul class="container">
                          <li><a href="https://stats.stackexchange.com/a/357974/301274" font size = "35">What is the difference Cross-entropy and KL divergence?</a></li>
                          <li><a href="https://distill.pub/2017/ctc/" font size = "35">Sequence Modeling with Connectionist Temporal Classification (CTC) Loss</a></li>
                        </ul>
                        <p>Encoder, Decoder, Autoencoder, Decoding, Sampling, Attention</p>
                        <ul class="container">
                          <li><a href="https://ai.stackexchange.com/questions/4245/what-is-the-difference-between-encoders-and-auto-encoders#:~:text=To%20answer%20this%20rather%20succinctly,is%20what%20the%20brain%20does.&text=An%20autoencoder%20will%20have%20the,to%20predict%20the%20Y%20target." font size = "35">What is the difference between encoders and auto-encoders?</a></li>
                          <li><a href="https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277" font size = "35">How to sample from language models - Temprature and Top k Sampling</a></li>
                          <li><a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" font size = "35">Seq2Seq with Attention and Beam Search</a></li>
                          <li><a href="https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/#:~:text=Attention%20is%20proposed%20as%20a,problem%20when%20decoding%20long%20sequences." font size = "35">How does Attention work in Encoder-Decoder Recurrent Neural Networks</a></li>
                          <li><a href="https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7" font size = "35">Beam Search Decoding in CTC-trained Neural Networks</a></li>
                          <li><a href="https://www.jeremyjordan.me/autoencoders/" font size = "35">Introduction to Autoencoders</a></li>
                          <li><a href="https://www.jeremyjordan.me/variational-autoencoders/" font size = "35">Introduction to Variational Autoencoders</a></li>
                          <li><a href="https://ark2210.medium.com/learning-disentangled-representations-with-variational-autoencoders-b1bfe237fffb" font size = "35">Learning Disentangled Representations with Variational Autoencoders</a></li>
                          <li><a href="https://github.com/facebookresearch/UnsupervisedMT" font size = "35">Unsupervised Machine Translation (Github)</a></li>
                        </ul>
                        <p>Machine Learning, Linear Algebra</p>
                        <ul class="container">
                          <li><a href="https://souravsengupta.com/cds2016/lectures/Savov_Notes.pdf" font size = "35">Linear Algebra exaplined in 4 pages</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/72087/r-squared-vs-t-test-confusion" font size = "35">R-squared vs. t-test confusion</a></li>
                          <li><a href="http://www.cs.columbia.edu/~verma/classes/ml/index.html" font size = "35">COMS 4771 Machine Learning</a></li>
                          <li><a href="https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves/7210#7210" font size = "35">ROC vs precision-recall curvers</a></li>
                          <li><a href="https://dustinstansbury.github.io/theclevermachine/bias-variance-tradeoff" font size = "35">Bias-Variance Tradeoff (Noise)</a></li>
                          <li><a href="http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/" font size = "35">Bias-Variance Decomposition - mlxtend</a></li>
                          <li><a href="https://towardsdatascience.com/introduction-to-optimization-with-genetic-algorithm-2f5001d9964b" font size = "35">Introduction to Optimization with Genetic Algorithm</a></li>
                          <li><a href="https://blog.zakjost.com/post/nce-intro/" font size = "35">Intro to Noise Contrastic Estimation</a></li>
                          <li><a href="https://machinelearningmastery.com/difference-test-validation-datasets/" font size = "35">Difference Between Test and Validation Datasets</a></li>
                        </ul>
                        <p>Deep Learning</p>
                        <ul class="container">
                          <li><a href="https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404" font size = "35">Weight Initializers (Random, Normal, Xavier, He)</a></li>
                          <li><a href="https://ruder.io/optimizing-gradient-descent/index.html" font size = "35">An Overview of Gradient Descent Optimization Algorithms</a></li>
                          <li><a href="https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html" font size = "35">Neural Architecture Search</a></li>
                          <li><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" font size = "35">A Step by Step Backpropagation Example</a></li>
                          <li><a href="Incorporating Nesterov Momentum into Adam" font size = "35">Incorporating Nesterov Momentum into Adam</a></li>
                          <li><a href="https://atcold.github.io/pytorch-Deep-Learning/" font size = "35">Deep Learning with PyTorch - Open Source Course by Yann LeCun and NYU</a></li>
                          <li><a href="https://homes.cs.washington.edu/~marcotcr/blog/lime/" font size = "35">LIME - Local Interpretable Model-Agnostic Explanations</a></li>
                        </ul>
                        <p>Miscellaneous - NLP</p>
                        <ul class="container">
                          <li><a href="https://monkeylearn.com/blog/introduction-to-topic-modeling/#what-is-topic-modeling" font size = "35">Topic Modeling</a></li>
                          <li><a href="https://github.com/SophonPlus/ChineseNlpCorpus" font size = "35">Chinese NLP Corpus</a></li>
                          <li><a href="https://aideadlin.es/?sub=ML,NLP" font size = "35">NLP & ML Conference Deadlines</a></li>
                          <li><a href="https://towardsdatascience.com/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc" font size = "35">The relationship between Perplexity and Entropy in NLP</a></li>
                          <li><a href="https://stackabuse.com/python-for-nlp-topic-modeling/" font size = "35">Python for NLP Topic Modeling</a></li>
                          <li><a href="https://github.com/joewandy/hlda" font size = "35">Hierarchical Latent Dirichlet Allocation (hLDA) for Topic Modeling - Github</a></li>
                          <li><a href="https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/" font size = "35">Topic Modeling Gensim Python</a></li>
                          <li><a href="https://github.com/niderhoff/nlp-datasets" font size = "35">Free NLP Datasets</a></li>
                          <li><a href="https://albertauyeung.github.io/2018/06/03/generating-ngrams.html" font size = "35">Generating N-grams from Sentences in Python</a></li>
                        </ul>
                        <p>Open Source Implementations - NLP</p>
                        <ul class="container">
                          <li><a href="https://github.com/Embedding/Chinese-Word-Vectors" font size = "35">Chinese Word Vectors</a></li>
                          <li><a href="https://github.com/OpenNMT/OpenNMT-py" font size = "35">OpenNMT-py: Open-Source Neural Machine Translation in PyTorch</a></li>
                          <li><a href="https://github.com/facebookresearch/UnsupervisedMT" font size = "35">Unsupervised Machine Translation</a></li>
                          <li><a href="https://github.com/facebookresearch/fastText" font size = "35">Facebook Research: fastText</a></li>
                          <li><a href="https://github.com/victoresque/pytorch-template" font size = "35">PyTorch Template Project</a></li>
                          <li><a href="http://www.cleverhans.io/" font size = "35">Cleverhans Blog - Library for Benchmarking the Vulnerability of Machine Learning models to Adversarial Examples</a></li>
                          <li><a href="https://github.com/interpretml/interpret-text" font size = "35">Interpret-Text - Github</a></li>
                          <li><a href="https://github.com/lilianweng/stock-rnn" font size = "35">Predict stock market prices using RNN</a></li>
                          <li><a href="https://github.com/justmarkham/scikit-learn-tips" font size = "35">Daily Scikit-Learn Tips</a></li>
                        </ul>
                        <p>Logistics: Conda, Python, Tensorboard, Jupyter</p>
                        <ul class="container">
                          <li><a href="https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf" font size = "35">Conda Cheatsheet</a></li>
                          <li><a href="https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514" font size = "35">Tensorboard Logging</a></li>
                          <li><a href="https://stackoverflow.com/a/55281501" font size = "35">How to accumulate gradients for large batch sizes in Keras</a></li>
                          <li><a href="https://medium.com/google-cloud/running-jupyter-notebooks-on-gpu-on-google-cloud-d44f57d22dbd" font size = "35">Running Jupyter Notebooks on GPU on Google Cloud</a></li>
                          <li><a href="https://github.com/naming-convention/naming-convention-guides/tree/master/python" font size = "35">Python Naming Convention Guides</a></li>
                          <li><a href="https://lucene.apache.org/solr/guide/8_7/solr-tutorial.html" font size = "35">Apache Solr Tutorial</a></li>
                        </ul>
                        <p>Papers</p>
                        <ul class="container">
                          <li><a href="https://arxiv.org/abs/2004.04906" font size = "35">Dense Passage Retrieval for Open-Domain Question Answering</a></li>
                          <li><a href="https://arxiv.org/abs/1711.00043" font size = "35">Unsupervised Machine Translation Using Monolingual Corpora Only</a> <a href="https://github.com/facebookresearch/UnsupervisedMT" font size = "35">(Github)</a></li>
                          <li><a href="https://www.aclweb.org/anthology/2020.acl-demos.40.pdf" font size = "35">NSTM: Real-Time Query-Driven News Overview Composition at Bloomberg</a></li>
                          <li><a href="https://arxiv.org/abs/1907.03040" font size = "35">BERT-DST: Scalable E2E Dialogue State Tracking with BERT</a></li>
                          <li><a href="https://github.com/tomohideshibata/BERT-related-papers" font size = "35">BERT-Related papers</a></li>
                          <li><a href="https://www.arxiv-vanity.com/papers/1906.05743/" font size = "35">Contrastive Bidirectional Transformer for Temporal Representation Learning</a></li>
                          <li><a href="https://arxiv.org/abs/1810.09587" font size = "35">StateNet: Towards Universal Dialogue State Tracking</a></li>
                          <li><a href="https://arxiv.org/abs/1908.01946" font size = "35">Dialog State Tracking: A Neural Reading Comprehension Approach</a></li>
                          <li><a href="https://arxiv.org/pdf/1911.04252.pdf" font size = "35">Self-training with Noisy Student improves ImageNet classification</a></li>
                          <li><a href="https://arxiv.org/pdf/1607.02533.pdf" font size = "35">Adversarial Examples In The Physical World</a></li>
                          <li><a href="https://arxiv.org/pdf/1708.04552.pdf" font size = "35">Improved Regularization of Convolutional Neural Networks with Cutout</a></li>
                          <li><a href="https://paperswithcode.com/task/dialogue-state-tracking" font size = "35">Dialgoue State Tracking SOTA</a></li>
                        </ul>
                        <p>CNN, CV</p>
                        <ul class="container">
                          <li><a href="https://cs231n.github.io/convolutional-networks/" font size = "35">CS231n Convolutional Neural Networks for Visual Recognition</a></li>
                          <li><a href="https://deeplizard.com/learn/video/0LhiS6yu2qQ" font size = "35">CNN Confusion Matrix With PyTorch - Neural Network Programming</a></li>
                          <li><a href="https://github.com/kubeflow-kale/examples/blob/master/pytorch-classification/cifar10_classification.ipynb" font size = "35">Cifar10 Classification Jupyter Notebook</a></li>
                          <li><a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py" font size = "35">PyTorch: Resnet</a></li>
                          <li><a href="https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py" font size = "35">Keras: Resnet on Cifar10</a></li>
                          <li><a href="https://github.com/uzaymacar/self-supervision" font size = "35">Self-Supervised Learning for Computer Vision (Github)</a></li>
                        </ul>
                        <p>Speech and ASR</p>
                        <ul class="container">
                          <li><a href="https://neuron.eng.wayne.edu/tarek/MITbook/chap5/5_4.html" font size = "35">Time-Delay Neural Network</a></li>
                          <li><a href="https://github.com/iamyuanchung/speech2vec-pretrained-vectors" font size = "35">Speech2Vec Pretrained Vectors (Github)</a></li>
                          <li><a href="https://verbyx.com/2020/01/13/understanding-speech-recognition/" font size = "35">Understanding Speech Recognition</a></li>
                          <li><a href="https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6#:~:text=Kaldi%20is%20an%20open%20source,speaker%20recognition%20and%20speaker%20diarisation.&text=Kaldi%20is%20written%20mainly%20in,with%20Bash%20and%20Python%20scripts." font size = "35">How to start with Kaldi and Speech Recognition</a></li>
                          <li><a href="http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" font size = "35">Mel Frequency Cepstral Coefficient (MFCC) Tutorial</a></li>
                          <li><a href="http://kaldi-asr.org/doc/data_prep.html" font size = "35">Kaldi Data Preparation</a></li>
                          <li><a href="http://kaldi-asr.org/doc/kaldi_for_dummies.html" font size = "35">Kaldi for Dummies Tutorial</a></li>
                          <li><a href="https://labrosa.ee.columbia.edu/matlab/rastamat/" font size = "35">MFCC in Matlab</a></li>
                          <li><a href="https://github.com/jameslyons/python_speech_features" font size = "35">MFCC in Python (Github)</a> - <a href="https://python-speech-features.readthedocs.io/en/latest/" font size = "35">Documentation</a></li>
                          <li><a href="https://github.com/YoavRamon/awesome-kaldi" font size = "35">Awesome Kaldi (Github)</a></li>
                          <li><a href="https://www.eleanorchodroff.com/tutorial/kaldi/" font size = "35">Great Kaldi Tutorial</a></li>
                        </ul>
        </body>
</html>
